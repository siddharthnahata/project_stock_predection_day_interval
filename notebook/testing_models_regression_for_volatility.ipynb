{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fff2e3a",
   "metadata": {},
   "source": [
    "## **1. This notebook is for model testing and finding best code and model for Garch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2bba3",
   "metadata": {},
   "source": [
    "**Importing necessary library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f669d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# for fetching data\n",
    "import yfinance as yf\n",
    "\n",
    "import ta # for indicator feature creation\n",
    "\n",
    "# machine learning tools/functions/objects\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# models\n",
    "from xgboost import XGBRegressor\n",
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d599b",
   "metadata": {},
   "source": [
    "### **1.1 --> fetching data and looking data creating new feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e8dc7",
   "metadata": {},
   "source": [
    "#### 1.11 fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3022890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Apple stock (daily)\n",
    "ticker = yf.Ticker(\"AAPL\")\n",
    "df = ticker.history(interval=\"1d\", period=\"5y\")\n",
    "df.drop([\"Dividends\", \"Stock Splits\"], axis=1, inplace=True) # droping unnecessary columns for our project\n",
    "df.index = df.index.strftime(\"%Y-%m-%d\") # changing the data format to yyyy-mm-dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358a9e8",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Used yfinance library(yahoo finance) to fetch stock historical data.\n",
    "* Droped unnecessary columns [\"Dividends\", \"Stock Splits\"] as they are not required for our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5dc22",
   "metadata": {},
   "source": [
    "#### 1.12 creating target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4bc8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a target column logic: taking tommorow return on today's row which make it target for next day prediction\n",
    "\n",
    "# Compute daily returns\n",
    "df['return'] = df['Close'].pct_change()\n",
    "df['target'] = df['return'].shift(-1)  # tomorrow’s return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c7585",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* created return column with help of pandas feature pct_change() which take last as base and current as comparing value.\n",
    "* shifted one column above as it will act as target to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a767a5",
   "metadata": {},
   "source": [
    "#### 1.13 creating new and extra useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1f0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rsi'] = ta.momentum.RSIIndicator(df['Close']).rsi()\n",
    "df['macd'] = ta.trend.MACD(df['Close']).macd()\n",
    "df['sma5'] = df['Close'].rolling(5).mean()\n",
    "df['sma10'] = df['Close'].rolling(10).mean()\n",
    "df['sma20'] = df['Close'].rolling(20).mean()\n",
    "df['sma50'] = df['Close'].rolling(50).mean()\n",
    "df['sma100'] = df['Close'].rolling(100).mean()\n",
    "df['sma200'] = df['Close'].rolling(200).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf76b3",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* RSI measures momentum by comparing recent gains and losses. It helps identify overbought (>70) or oversold (<30) conditions, guiding traders on potential reversal points or trend continuation strength.\n",
    "* MACD highlights trend direction and momentum by comparing two moving averages. Its crossovers, divergence, and histogram provide insights into potential entry and exit points, helping traders spot trend reversals or confirm ongoing trends.\n",
    "* SMA smooths price data by averaging over a set period, filtering noise and showing overall trend direction. It acts as dynamic support/resistance, and crossovers with other SMAs often signal buy/sell opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677d227",
   "metadata": {},
   "source": [
    "### **1.2 --> Defing X, y and spliting data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa55af0",
   "metadata": {},
   "source": [
    "#### 1.21 defining X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3867dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df[['rsi','macd','sma5', 'Volume', 'sma10', 'sma20', 'sma50', 'sma100', 'sma200']]  # features\n",
    "y = df['target']                # tomorrow’s return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff584ff5",
   "metadata": {},
   "source": [
    "#### 1.22 Spliting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da258b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e5e6a",
   "metadata": {},
   "source": [
    "### **1.3 --> Training models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b36db",
   "metadata": {},
   "source": [
    "#### 1.31 Implementing one of the best regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da243fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0006186901586766389\n",
      "MAE : 0.017666524648262617\n",
      "R_Squared : -0.3147139846747442\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse = mean_squared_error(y_test, pred)\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "r_square = r2_score(y_test, pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE :\", mae)\n",
    "print(\"R_Squared :\", r_square)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187fa7b",
   "metadata": {},
   "source": [
    "#### 1.32 Implementing one of the best model for this problem statement Grach with hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87aeed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: ['EGARCH', 't', 2, 1] AIC: 3989.7218697832236 QLIKE: 0.7459978938506164\n",
      "                        Constant Mean - EGARCH Model Results                        \n",
      "====================================================================================\n",
      "Dep. Variable:                        Close   R-squared:                       0.000\n",
      "Mean Model:                   Constant Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                           EGARCH   Log-Likelihood:               -1988.86\n",
      "Distribution:      Standardized Student's t   AIC:                           3989.72\n",
      "Method:                  Maximum Likelihood   BIC:                           4019.48\n",
      "                                              No. Observations:                 1054\n",
      "Date:                      Tue, Sep 23 2025   Df Residuals:                     1053\n",
      "Time:                              18:31:56   Df Model:                            1\n",
      "                                Mean Model                                \n",
      "==========================================================================\n",
      "                 coef    std err          t      P>|t|    95.0% Conf. Int.\n",
      "--------------------------------------------------------------------------\n",
      "mu             0.1035  4.442e-02      2.331  1.978e-02 [1.646e-02,  0.191]\n",
      "                              Volatility Model                              \n",
      "============================================================================\n",
      "                 coef    std err          t      P>|t|      95.0% Conf. Int.\n",
      "----------------------------------------------------------------------------\n",
      "omega          0.0238  1.017e-02      2.341  1.925e-02 [3.872e-03,4.375e-02]\n",
      "alpha[1]       0.3208  7.967e-02      4.026  5.662e-05     [  0.165,  0.477]\n",
      "alpha[2]      -0.2034  8.478e-02     -2.399  1.645e-02  [ -0.370,-3.720e-02]\n",
      "beta[1]        0.9845  7.872e-03    125.057      0.000     [  0.969,  1.000]\n",
      "                              Distribution                              \n",
      "========================================================================\n",
      "                 coef    std err          t      P>|t|  95.0% Conf. Int.\n",
      "------------------------------------------------------------------------\n",
      "nu             5.0889      0.791      6.434  1.244e-10 [  3.539,  6.639]\n",
      "========================================================================\n",
      "\n",
      "Covariance estimator: robust\n"
     ]
    }
   ],
   "source": [
    "returns = 100 * df['Close'].pct_change().dropna()  # creating data for fitting model\n",
    "\n",
    "def qlike_loss(realized_var, predicted_var):\n",
    "    return np.mean((realized_var / predicted_var) - np.log(realized_var / predicted_var) - 1)\n",
    "\n",
    "best_aic, best_model, best_params, best_qlike = 1e10, None, None, None\n",
    "\n",
    "for vol in [\"Garch\", \"EGARCH\", \"GJR-GARCH\"]:\n",
    "    for dist in [\"normal\", \"t\", \"skewt\"]:\n",
    "        for p in range(1,3):\n",
    "            for q in range(1,3):\n",
    "                try:\n",
    "                    model = arch_model(returns, vol=vol, p=p, q=q, dist=dist, mean=\"Constant\")\n",
    "                    res = model.fit(disp=\"off\")\n",
    "                    forecast = res.forecast(horizon=1)\n",
    "                    predicted_var = forecast.variance.iloc[-1].values\n",
    "                    realized_var = returns.iloc[-1]**2\n",
    "                    qlike = qlike_loss(np.array([realized_var]), predicted_var)\n",
    "                    \n",
    "                    if res.aic < best_aic:\n",
    "                        best_aic = res.aic\n",
    "                        best_model = res\n",
    "                        best_params = [vol, dist, p, q]\n",
    "                        best_qlike = qlike\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "print(\"Best Model:\", best_params, \"AIC:\", best_aic, \"QLIKE:\", best_qlike)\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189442c9",
   "metadata": {},
   "source": [
    "#### **Note:**\n",
    "The GARCH model outperforms traditional regressor models in predicting stock market volatility because it directly captures time-varying variance, volatility clustering, and persistence in financial returns, while regressors often fail to adapt dynamically to changing market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46ed59",
   "metadata": {},
   "source": [
    "#### 1.33 code for next point prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9ec022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted variance: [3.71519344]\n",
      "Predicted volatility (%): [1.92748371]\n"
     ]
    }
   ],
   "source": [
    "model = arch_model(returns, vol=best_params[0], p=best_params[2], q=best_params[3], dist=best_params[1], mean=\"Constant\")\n",
    "res = model.fit(disp=\"off\")\n",
    "\n",
    "# Forecast next 5 days ahead\n",
    "forecast = res.forecast(horizon=1)\n",
    "\n",
    "# Extract variance forecast\n",
    "predicted_var = forecast.variance.iloc[-1].values\n",
    "predicted_vol = np.sqrt(predicted_var)\n",
    "\n",
    "print(\"Predicted variance:\", predicted_var)\n",
    "print(\"Predicted volatility (%):\", predicted_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae6dcd",
   "metadata": {},
   "source": [
    "**Best Model:** `['EGARCH', 't', 2, 1]`  \n",
    "**AIC:** 3993.16  \n",
    "**Log-Likelihood:** -1990.58  \n",
    "**Observations:** 1055  \n",
    "\n",
    "---\n",
    "\n",
    "### Mean Model\n",
    "- **Mean Type:** Constant Mean  \n",
    "- **mu (μ):** 0.1049 (p < 0.001) → Significant positive mean return  \n",
    "\n",
    "---\n",
    "\n",
    "### Volatility Model (EGARCH)\n",
    "- **omega (ω):** 0.0238 (p = 0.0196) → Baseline variance  \n",
    "- **alpha[1]:** 0.3207 (p < 0.001) → Strong impact of shocks on volatility  \n",
    "- **alpha[2]:** -0.2032 (p = 0.0157) → Asymmetry (negative shocks reduce volatility)  \n",
    "- **beta[1]:** 0.9844 (p < 0.001) → High persistence of volatility  \n",
    "\n",
    "---\n",
    "\n",
    "### Distribution (Student’s t)\n",
    "- **nu (ν):** 5.1075 (p < 0.001) → Heavy tails, capturing fat-tailed return distributions  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "- EGARCH effectively captures volatility clustering and asymmetry in stock returns.  \n",
    "- High **β (0.9844)** indicates persistent volatility shocks.  \n",
    "- Student’s t distribution supports modeling of fat tails in financial data.  \n",
    "- Overall, this EGARCH model is superior for volatility forecasting compared to simple regressor models, as it accounts for **time-varying variance, persistence, and asymmetry**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf426471",
   "metadata": {},
   "source": [
    "## Why Garch Models and not Regression Models.\n",
    "\n",
    "\n",
    "GARCH models are superior for volatility prediction because they account for volatility clustering—periods of high volatility tend to be followed by more high volatility, and vice versa. Unlike simple regression models, GARCH captures this dynamic by making the conditional variance dependent on past squared errors (ARCH component) and past estimated variances (GARCH component). This makes GARCH models more realistic for financial time series, which exhibit changing variances and are not well-represented by constant variance assumptions of standard regressions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
